{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import kscope\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with role-play prompting, as detailed in the paper [Better Zero-Shot Reasoning with Role-Play Prompting](https://arxiv.org/pdf/2308.07702.pdf). The idea is fairly simple. Suppose we want the language model to perform a certain task T. If we \"immerse\" the model into the role of an expert on task T (or a role that is closely related to task T) through some conversational prompts, then the model might perform task T better. \n",
    "\n",
    "Following (roughly) the paper, we will compare role-play prompting with two other methods: Zero-shot prompting and Zero-shot CoT on the task of solving math word problems using the MultiArith dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '93d68514-a20b-4177-8ab3-a7de3816fc01',\n",
       "  'name': 'llama2-70b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the LLaMA-2 70B model.\n",
    "\n",
    "__Note__: LLaMA-2 70B is large, so prompt generation may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama2-70b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`\n",
    "\n",
    "Note that here we set the temperature to 0.5, meaning we are not using greedy decoding, contrary to the experiments conducted in the paper above. This is done because we have observed that setting the temperature to be positive produces slightly better performance for the role-play prompting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 75, \"top_k\": 4, \"top_p\": 1.0, \"temperature\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiArith: Math Word Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiArith is a dataset consisting of math word problems. The following function is used to read the data from the raw json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(dataset_path: str) -> Tuple[List[str], List[str]]:\n",
    "    questions = []\n",
    "    answers = []\n",
    "    with open(dataset_path) as f:\n",
    "        json_data = json.load(f)\n",
    "        for line in json_data:\n",
    "            q = line[\"sQuestion\"].strip()\n",
    "            a = str(line[\"lSolutions\"][0])\n",
    "            if a[-2:] == \".0\":\n",
    "                a = a[:-2]\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example question and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For Halloween Debby and her sister combined the candy they received. Debby had 32 pieces of candy while her sister had 42. If they ate 35 pieces the first night, how many pieces do they have left?\n",
      "Correct answer: 39\n"
     ]
    }
   ],
   "source": [
    "questions, answers = data_reader(\"resources/multi_arith_dataset/MultiArith.json\")\n",
    "\n",
    "print(f\"Question: {questions[0]}\")\n",
    "print(f\"Correct answer: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly sample a subset and use it for evaluation since the whole dataset is quite large and prompting takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "sample_questions_interm, sample_answers_interm = zip(*random.sample(list(zip(questions, answers)), num_samples))\n",
    "sample_questions = list(sample_questions_interm)\n",
    "sample_answers = [int(answer) for answer in list(sample_answers_interm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets for MultiArith are integers, and since the model would likely produce answers that are not integers, we need a way to extract a numerical answer from the model's response string. We perform this by another round of prompting. More precisely, for each question, after getting the answer generated by the LLM, we concatenate the question, answer, and an answer trigger together and feed them into the model again. \n",
    "\n",
    "For MultiArith specifically, we use the answer trigger \"Therefore, the answer (arabic numerals) is\" and we extract the first integer in the model's response to the second prompt as its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numerical_answer(response_str: str) -> int:\n",
    "    response_str = response_str.replace(\",\", \"\")\n",
    "    response_str = response_str.replace(\".\", \"\")\n",
    "    numbers = [s for s in re.findall(r\"-?\\d+\\.?\\d*\", response_str)]\n",
    "    if len(numbers) > 0:\n",
    "        return int(numbers[0])\n",
    "    else:\n",
    "        # If the model response does not contain any number, we just return a random integer.\n",
    "        return random.randint(3000, 3100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try Zero-shot prompting with no CoT. We will begin by storing all the model's responses to the raw math questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [13:44<00:00, 16.49s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_responses = []\n",
    "for sample_question in tqdm(sample_questions_interm):\n",
    "    generation = model.generate(sample_question, long_generation_config)\n",
    "    zero_shot_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ An example question and the model response ============\n",
      "Question: Sarah had 55 homework problems. She finished 6 of them but still had 7 pages of problems to do. If each page has the same number of problems on it, how many problems are on each page?\n",
      "Correct answer: 7\n",
      "Model response:\n",
      "Sarah had 55 homework problems. She finished 6 of them but still had 7 pages of problems to do. If each page has the same number of problems on it, how many problems are on each page?...\n",
      "What is the area of a square with a side length of 10 units?\n",
      "What is the area of a\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(\"============ An example question and the model response ============\")\n",
    "print(f\"Question: {sample_questions_interm[i]}\")\n",
    "print(f\"Correct answer: {sample_answers[i]}\")\n",
    "print(f\"Model response:{zero_shot_responses[i]['sequences'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate each one of the model's responses with the answer trigger and feed the result into the model again in order to extract numerical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_answer_trigger = \"Therefore, the answer (arabic numerals) is\"\n",
    "multi_arith_concatenated_prompts = []\n",
    "second_generations = []\n",
    "for sample_question, model_response in zip(sample_questions_interm, zero_shot_responses):\n",
    "    response_str = model_response[\"sequences\"][0]\n",
    "    concatenated_prompt = f\"{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ An example question and the model response to the concatenated prompt ============\n",
      "Question: Sarah had 55 homework problems. She finished 6 of them but still had 7 pages of problems to do. If each page has the same number of problems on it, how many problems are on each page?\n",
      "Correct answer: 7\n",
      "Second round prompt: Sarah had 55 homework problems. She finished 6 of them but still had 7 pages of problems to do. If each page has the same number of problems on it, how many problems are on each page?'\n",
      "'\n",
      "Sarah had 55 homework problems. She finished 6 of them but still had 7 pages of problems to do. If each page has the same number of problems on it, how many problems are on each page?...\n",
      "What is the area of a square with a side length of 10 units?\n",
      "What is the area of a'\n",
      "'Therefore, the answer (arabic numerals) is\n",
      "Model response:the area of a square with a side length of 10 units. What number is represented by this symbol?\n",
      "Therefore, the answer (arabic numerals) is the area of a square with a side length of 10 units. What number is represented by this symbol?...\n",
      "'Therefore, the answer (arabic numerals) is the\n"
     ]
    }
   ],
   "source": [
    "print(\"============ An example question and the model response to the concatenated prompt ============\")\n",
    "print(f\"Question: {sample_questions_interm[i]}\")\n",
    "print(f\"Correct answer: {sample_answers[i]}\")\n",
    "print(f\"Second round prompt: {multi_arith_concatenated_prompts[i]}\")\n",
    "print(f\"Model response:{second_generations[i]['sequences'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the extracted numerical answers with the correct answers to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n"
     ]
    }
   ],
   "source": [
    "final_answers = [\n",
    "    extract_numerical_answer(second_generation[\"sequences\"][0]) for second_generation in second_generations\n",
    "]\n",
    "accuracy = np.sum(np.array(final_answers) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is not very good. Next, we will follow the same evaluation procedure to test the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try Zero-shot CoT as another baseline for comparison. This is pretty similar to the vanilla zero-shot approach above, except we append the sentence \"Let's think step by step\" to the end of each prompt to encourage the model to perform CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_post_prompt = \"Let's think step by step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:07<00:00, 16.94s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_cot_responses = []\n",
    "for sample_question in tqdm(sample_questions_interm):\n",
    "    prompt = f\"{sample_question}\\n{cot_post_prompt}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    zero_shot_cot_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_cot = []\n",
    "second_generations_cot = []\n",
    "for sample_question, model_response in zip(sample_questions_interm, zero_shot_cot_responses):\n",
    "    response_str = model_response[\"sequences\"][0]\n",
    "    concatenated_prompt = f\"{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_cot.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_cot.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "final_answers_cot = [\n",
    "    extract_numerical_answer(second_generation[\"sequences\"][0]) for second_generation in second_generations_cot\n",
    "]\n",
    "accuracy = np.sum(np.array(final_answers_cot) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 3070, 12, 1, 27, 49, 54, 4, 64, 36, 7, 9, 34, 56, 6, 48, 1, 6, 7, 63, 20, 10, 3, 3030, 27, 6, 19, 7, 6, 45, 2, 15, 36, 55, 3053, 24, 46, 9, 53, 60, 24, 24, 22, 18, 7, 86, 3057, 51, 3018, 10]\n",
      "[28, 9, 48, 30, 27, 49, 54, 2, 32, 72, 7, 8, 34, 56, 6, 48, 60, 2, 9, 63, 20, 10, 6, 1, 27, 48, 19, 7, 9, 45, 2, 15, 36, 11, 9, 30, 26, 9, 50, 60, 72, 23, 24, 18, 7, 7, 16, 9, 4, 10]\n"
     ]
    }
   ],
   "source": [
    "print(final_answers_cot)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adding the simple sentence \"Let's think step by step.\", CoT induces a quite significant improvement in performance. Now let's see whether role-play prompting works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role-play Prompting\n",
    "\n",
    "We consider three different role-play prompts, representing three different levels of immersion. \n",
    "\n",
    "In the first level, we simply inform the model of the role it is expected to play before asking the question.\n",
    "\n",
    "In the second level, immersion is enhanced by adding complementary descriptions of the role in the prompt.\n",
    "\n",
    "In the third level, we append to the end of the level-2 prompt a \"response\" in which the model acknowledges the role it plays. Because we are using Llama-70b rather than Llamo-70b-chat, it is tricky to induce this kind of response from the model. So instead, we just artificially create the response and concatenate it with the level-2 prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt_level1 = \"From now on, you are a math teacher. Please answer the following question\"\n",
    "role_prompt_level2 = \"\"\"From now on, you are an excellent math teacher and always teach your\n",
    "students math problems correctly. I am one of your students and ask you the following question.\"\"\"\n",
    "role_prompt_level3 = \"\"\"From now on, you are an excellent math teacher and always teach your\n",
    "students math problems correctly. And I am one of your students. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:13<00:00, 17.08s/it]\n"
     ]
    }
   ],
   "source": [
    "level1_responses = []\n",
    "for sample_question in tqdm(sample_questions_interm):\n",
    "    prompt = f\"{role_prompt_level1}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level1_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level1 = []\n",
    "second_generations_level1 = []\n",
    "for sample_question, model_response in zip(sample_questions_interm, level1_responses):\n",
    "    response_str = model_response[\"sequences\"][0]\n",
    "    concatenated_prompt = f\"{role_prompt_level1}\\n{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_level1.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level1.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.18\n"
     ]
    }
   ],
   "source": [
    "final_answers_level1 = [\n",
    "    extract_numerical_answer(second_generation[\"sequences\"][0]) for second_generation in second_generations_level1\n",
    "]\n",
    "accuracy = np.sum(np.array(final_answers_level1) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply informing the model of the role we wish it to play does not seem to help. We can try printing out some of the model's responses to our role-play prompt to see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a math teacher. Please answer the following question.\n",
      "You have 15 apples. Your brother ate 7 of them. You ate 4 of them. How many apples do you have left?\n",
      "You are now a math teacher. Please answer the following question.\n",
      "You have 20 apples. Your brother ate\n",
      "[13, 10, 3000, 19, 34, 3049, 12, 1, 8, 132, 8, 9, 48, 70, 6, 3100, 31, 1, 3099, 63, 3000, 10, 6, 3035, 5, 6, 3021, 7, 33, 3040, 3034, 19, 35, 43, 9, 10, 3008, 3056, 14, 3063, 45, 31, 22, 3024, 3050, 7, 12, 3089, 4, 10]\n",
      "[28, 9, 48, 30, 27, 49, 54, 2, 32, 72, 7, 8, 34, 56, 6, 48, 60, 2, 9, 63, 20, 10, 6, 1, 27, 48, 19, 7, 9, 45, 2, 15, 36, 11, 9, 30, 26, 9, 50, 60, 72, 23, 24, 18, 7, 7, 16, 9, 4, 10]\n"
     ]
    }
   ],
   "source": [
    "print(level1_responses[6][\"sequences\"][0])\n",
    "print(final_answers_level1)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:16<00:00, 17.13s/it]\n"
     ]
    }
   ],
   "source": [
    "level2_responses = []\n",
    "for sample_question in tqdm(sample_questions_interm):\n",
    "    prompt = f\"{role_prompt_level2}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level2_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level2 = []\n",
    "second_generations_level2 = []\n",
    "for sample_question, model_response in zip(sample_questions_interm, level2_responses):\n",
    "    response_str = model_response[\"sequences\"][0]\n",
    "    concatenated_prompt = f\"{role_prompt_level2}\\n{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_level2.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level2.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "final_answers_level2 = [\n",
    "    extract_numerical_answer(second_generation[\"sequences\"][0]) for second_generation in second_generations_level2\n",
    "]\n",
    "accuracy = np.sum(np.array(final_answers_level2) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that adding more detailed descriptions to the role slightly increases the performance. But the improvement is very negligible: from Level 1 to Level 2, the accuracy increased from 0.18 to 0.2, which is still lower than the accuracy of vanilla Zero-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I know that you will teach me the correct answer and the correct method to solve this problem.\n",
      "I will be very grateful if you can teach me the correct method to solve this problem.\n",
      "Thank you very much for your time and patience!\n",
      "Best regards, Winston.\n",
      "Re: Math Problem\n",
      "Hey Winston.\n",
      "The answer is\n",
      "[3075, 50, 3100, 3008, 13, 3052, 54, 14, 36, 1, 13, 3065, 16, 128, 4, 54, 3068, 3066, 10, 57, 24, 2, 3085, 0, 27, 3053, 19, 1000, 1, 72, 3073, 3036, 36, 3074, 3001, 16, 36, 2, 56, 30, 3030, 21, 2, 3016, 17, 7, 3090, 5, 3091, 20]\n",
      "[28, 9, 48, 30, 27, 49, 54, 2, 32, 72, 7, 8, 34, 56, 6, 48, 60, 2, 9, 63, 20, 10, 6, 1, 27, 48, 19, 7, 9, 45, 2, 15, 36, 11, 9, 30, 26, 9, 50, 60, 72, 23, 24, 18, 7, 7, 16, 9, 4, 10]\n"
     ]
    }
   ],
   "source": [
    "print(level2_responses[6][\"sequences\"][0])\n",
    "print(final_answers_level2)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how well level-3 role-play prompting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the artificial response we will append to the role prompt. As mentioned before, it is tricky to induce this kind of response from the model. \n",
    "\n",
    "We also print out an example of the mode's actual response. As we can see, unlike in the artificial response, the model does not acknowledge its role as a math teacher, likely due to the fact that it is not a \"chat-tuned\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt_level3_follow_up = \"\"\"That’s great to hear! As your math teacher,\n",
    "I’ll do my best to explain mathematical concepts correctly so that you can understand them easily.\n",
    "Feel free to ask any math problems or questions you have, and I’ll be glad to assist you.\n",
    "Let’s dive into the world of mathematics and explore its wonders together!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the raw response the model provides to being informed out its role as a Math Teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😉\n",
      "I’m glad you have been able to find some useful resources.\n",
      "As for the “teacher” thing, I am not sure if it is a good thing or a bad thing. 🙂\n",
      "I think it is a good thing. 😉\n",
      "Let’s be honest, I am not a math\n"
     ]
    }
   ],
   "source": [
    "generation_level3_role_prompt = model.generate(role_prompt_level3, long_generation_config)\n",
    "print(generation_level3_role_prompt.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, rather than allowing a free form response, we inject it into the prompt and then as our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:14<00:00, 17.09s/it]\n"
     ]
    }
   ],
   "source": [
    "level3_responses = []\n",
    "for sample_question in tqdm(sample_questions_interm):\n",
    "    prompt = f\"{role_prompt_level3}\\n{role_prompt_level3_follow_up}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level3_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level3 = []\n",
    "second_generations_level3 = []\n",
    "for sample_question, model_response in zip(sample_questions_interm, level3_responses):\n",
    "    response_str = model_response[\"sequences\"][0]\n",
    "    concatenated_prompt = f\"\"\"{role_prompt_level3}\\n{role_prompt_level3_follow_up}\\n{\n",
    "        sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\"\"\n",
    "    multi_arith_concatenated_prompts_level3.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level3.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42\n"
     ]
    }
   ],
   "source": [
    "final_answers_level3 = [\n",
    "    extract_numerical_answer(second_generation[\"sequences\"][0]) for second_generation in second_generations_level3\n",
    "]\n",
    "accuracy = np.sum(np.array(final_answers_level3) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the fake response in which the model acknowledges its role improves the performance quite markedly. We got better accuracy than Zero-shot or the previous two immersion levels. Unfortunately, for this task, the accuracy is still slightly worse than Zero-shot CoT, but it does demonstrate how playing a role impressively improves how the model answers questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.\n",
      "I hope this helps!'\n",
      "'Therefore, the answer (arabic numerals) is 49.\n",
      "I hope this helps!'\n",
      "'Therefore, the answer (arabic numerals) is 49.\n",
      "I hope this helps!'\n",
      "'Therefore, the answer (arabic numerals) is 49\n",
      "[63, 8, 14, 30, 27, 49, 54, 41, 64, 36, 49, 9, 3075, 8, 6, 48, 60, 2, 9, 63, 24, 10, 5, 1, 1, 6, 2, 7, 197, 9, 20, 15, 36, 336, 13, 20, 3031, 12, 50, 60, 72, 23, 22, 12, 3011, 12, 39, 25, 4, 10]\n",
      "[28, 9, 48, 30, 27, 49, 54, 2, 32, 72, 7, 8, 34, 56, 6, 48, 60, 2, 9, 63, 20, 10, 6, 1, 27, 48, 19, 7, 9, 45, 2, 15, 36, 11, 9, 30, 26, 9, 50, 60, 72, 23, 24, 18, 7, 7, 16, 9, 4, 10]\n"
     ]
    }
   ],
   "source": [
    "print(second_generations_level3[5][\"sequences\"][0])\n",
    "print(final_answers_level3)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper provided some evidence that role-play prompting can work better than CoT in many tasks, but in their case the model was able to acknowledge its role on its own, so there was no need to artificially create a fake response. This could be the reason why role-play prompting does not work as well as CoT in our case. Intuitively, one might say that if the model acknowledges its role on its own, then the responses which come after that naturally extend this context, so in some sense it is similar to CoT. \n",
    "\n",
    "We can also observe that in Levels 1 and 2, the model gave some incoherent responses which suggest it did not immerse itself in the role of a math teacher. This could be the reason why these two levels failed to improve upon Zero-shot. The model we are using here (LLaMA-2-70b) has not been fine-tuned for chat purposes, so it is harder to induce conversation-like behaviour from the model, which can be important for role-play prompting to work. A natural follow-up experiment one might want to run to verify this idea is to use a model that has been fine-tuned for chat purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
