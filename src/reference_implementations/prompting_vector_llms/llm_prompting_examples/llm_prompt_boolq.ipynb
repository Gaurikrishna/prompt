{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from random import sample\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import report_metrics\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, LlamaTokenizerFast\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '09987df5-eb97-41e0-8b58-d0c16b6555ac',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the LLaMA-2 model.\n",
    "\n",
    "**NOTE**: This notebook uses activation retrieval to extract responses from the model: \n",
    "* This functionality is available for LLaMA-2 models (non-chat). \n",
    "* It is **NOT**, however, currently available for Falcon models of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is active!\n"
     ]
    }
   ],
   "source": [
    "model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"The model is active!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have the model attempt to answer questions based on some context. The answer to each question is either yes or no based on the question and context. We're going to focus on zero-shot prompting for this task, as LLaMA-2 is fairly good at it and the examples are quite long, making few-shot prompting difficult. We'll consider a few-shot example at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_preprocessor(path: str) -> Tuple[List[str], List[str], List[str], List[int]]:\n",
    "    boolq_df = pd.read_csv(path)\n",
    "    titles = boolq_df[\"Title\"].tolist()\n",
    "    passages = boolq_df[\"Passage\"].tolist()\n",
    "    questions = boolq_df[\"Question\"].tolist()\n",
    "    labels = boolq_df[\"Answer\"].apply(lambda x: 1 if x else 0).tolist()\n",
    "    return titles, passages, questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a sampling of the BoolQ test dataset and a small sample of training examples from the training dataset for\n",
    "# few-shot prompting\n",
    "bool_q_test_titles, bool_q_test_passages, bool_q_test_questions, bool_q_test_labels = boolq_preprocessor(\n",
    "    \"resources/boolq_task_datasets/test_sample_dataset.csv\"\n",
    ")\n",
    "bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels = boolq_preprocessor(\n",
    "    \"resources/boolq_task_datasets/example_dataset.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be working on the BoolQ task. In this task, the model is given a passage that contains the answer to a question. The model should be able to answer the question based on the passage provided. The answer to the questions is either yes or no. An example is below.\n",
    "\n",
    "**Title**: Rabies transmission\n",
    "\n",
    "**Passage**: Transmission between humans is extremely rare, although it can happen through organ transplants, or through bites.\n",
    "\n",
    "**Question**: can a person transmit rabies to another person?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating prompts, demonstrations are used for few-shot examples. If demonstrations in the `create_prompts` function is an empty string then the prompt is zero shot (that is, it includes no demonstrations). We follow the prompt structure used by the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf) for the BoolQ task. That is \n",
    "\n",
    "{title} -- {passage}\n",
    "\n",
    "question: {question}\n",
    "\n",
    "answer: {answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(\n",
    "    demo_titles: List[str],\n",
    "    demo_passages: List[str],\n",
    "    demo_questions: List[str],\n",
    "    demo_labels: List[int],\n",
    "    label_map: Dict[int, str],\n",
    "    n_demos: Optional[int] = None,\n",
    ") -> List[str]:\n",
    "    # n_demos controls how many demonstration examples are included. That is, n_demo-shot prompts are created. If\n",
    "    # n_demos is none, all available demonstrations are used\n",
    "    demonstrations = []\n",
    "    for demo_title, demo_passage, demo_question, demo_label in zip(\n",
    "        demo_titles, demo_passages, demo_questions, demo_labels\n",
    "    ):\n",
    "        label_str = label_map[demo_label]\n",
    "        demonstration = f\"{demo_title} -- {demo_passage}\\nquestion: {demo_question}?\\nanswer: {label_str}\\n\\n\"\n",
    "        demonstrations.append(demonstration)\n",
    "    return sample(demonstrations, n_demos) if n_demos else demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the BoolQ task are fairly long and our implementation of LLaMA-2, which we are using here, has a cap on the input length (512). So it's difficult to pack in a lot of examples for this task. The `create_prompt` function below, will put as many of the demonstrations into each prompt as possible without over-running the required length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(\n",
    "    demonstrations: List[str],\n",
    "    test_titles: List[str],\n",
    "    test_passages: List[str],\n",
    "    test_questions: List[str],\n",
    "    tokenizer: Optional[LlamaTokenizerFast] = None,\n",
    ") -> List[str]:\n",
    "    prompts = []\n",
    "    for test_title, test_passage, test_question in zip(test_titles, test_passages, test_questions):\n",
    "        prompt = f\"{test_title} -- {test_passage}\\nquestion: {test_question}?\\nanswer:\"\n",
    "        n_shot = 0\n",
    "        # We all demonstrations or as many as we can, without violating the LLaMA max prompt length\n",
    "        for demonstration in demonstrations:\n",
    "            candidate_prompt = f\"{demonstration}{prompt}\"\n",
    "            if tokenizer is not None:\n",
    "                if len(tokenizer.encode(candidate_prompt)) < 512:\n",
    "                    n_shot += 1\n",
    "                    prompt = candidate_prompt\n",
    "                else:\n",
    "                    print(f\"Prompt is too long: {len(tokenizer.encode(candidate_prompt))}. Using {n_shot}-shot prompt\")\n",
    "                    break\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer activations of the model are analogous to the probabilities of each token in the model vocabulary. That is, it is the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a vector of dimension $32000$ (the vocabulary size of LLaMA-2) which encodes the probability distribution of $y_{t+1}$ over the vocabulary. For this example, we only care about the last token in our input, as it houses the probability of the, as yet, unseen token the model will generate.\n",
    "\n",
    "**NOTE**: The last layer for LLaMA-2, named \"output,\" is actually the logits (pre-softmax) and therefore not quite probabilities, but is proportional to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below, we're interested in the activations from the last layer of the model, because this will allow us to calculate\n",
    "# the likelihoods.\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to consider zero-shot prompting with two different generation configurations. For a discussion of possible configuration parameters see the [Configuration README](src/reference_implementations/prompting_vector_llms/CONFIG_README.md).\n",
    "\n",
    "For the first experiment, we'll consider sampling in our response generation, letting `temperature = 0.8`. In the second experiment, we'll narrow the generation settings to greedy decoding (`temperature = 0.0`), where the model always selects the most probable token, at least from it's perspective. In both cases, we'll just generate a single token, as we're looking for a yes/no answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_generation_config = {\"max_tokens\": 1, \"temperature\": 0.8}\n",
    "greedy_generation_config = {\"max_tokens\": 1, \"temperature\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting (Stochastic Generation)\n",
    "\n",
    "In this section, we won't include any demonstrations in our prompts and we'll sample from the answer distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "lowercase_labels = [label.lower() for label in label_ordering]\n",
    "prompts = create_prompts([], bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "# Let's check one of the prompts.\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [03:28<00:00, 20.82s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "unmatched_predictions = []\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    responses = model.generate(prompt_batch, stochastic_generation_config)\n",
    "    processed_responses = [generation.strip().lower() for generation in responses.generation[\"sequences\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in processed_responses:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            unmatched_predictions.append(potential_prediction)\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Responses did not match the label space ['no', 'yes']\n",
      "Some examples of unmatched responses: ['the', 'lead', 'the', 'fred', 'an', 'there', '', 'the', 'the', 'ty']\n",
      "Prediction Accuracy: 0.57\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[19 20]\n",
      " [23 38]]\n",
      "========================================================\n",
      "Label: no, F1: 0.4691358024691358, Precision: 0.4523809523809524, Recall: 0.48717948717948717\n",
      "Label: yes, F1: 0.6386554621848739, Precision: 0.6551724137931034, Recall: 0.6229508196721312\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(unmatched_predictions)} Responses did not match the label space {lowercase_labels}\")\n",
    "print(f\"Some examples of unmatched responses: {sample(unmatched_predictions, 10)}\")\n",
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label].lower() for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=lowercase_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stochastic sampling, it's pretty clear that the model doesn't always answer in our label space. This makes it hard to map the response to a prediction that we can score and we end up selecting a random guess a lot of the time. This leads us to have poor accuracy on the task. Let's see if we can do better with Greedy Decoding (setting `temperature = 0.0`)\n",
    "\n",
    "### Zero-shot Prompting (Greedy Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:22<00:00, 20.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setting random seet for consistent demonstration construction\n",
    "random.seed(2024)\n",
    "predicted_labels = []\n",
    "unmatched_predictions = []\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    responses = model.generate(prompt_batch, greedy_generation_config)\n",
    "    processed_responses = [generation.strip().lower() for generation in responses.generation[\"sequences\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in processed_responses:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            unmatched_predictions.append(potential_prediction)\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Responses did not match the label space ['no', 'yes']\n",
      "Some examples of unmatched responses: ['the', 'the', 'anne', 'a', 'oliv']\n",
      "Prediction Accuracy: 0.8\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[27 12]\n",
      " [ 8 53]]\n",
      "========================================================\n",
      "Label: no, F1: 0.7297297297297296, Precision: 0.7714285714285715, Recall: 0.6923076923076923\n",
      "Label: yes, F1: 0.8412698412698412, Precision: 0.8153846153846154, Recall: 0.8688524590163934\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(unmatched_predictions)} Responses did not match the label space {lowercase_labels}\")\n",
    "print(f\"Some examples of unmatched responses: {sample(unmatched_predictions, 5)}\")\n",
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label].lower() for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=lowercase_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite not properly matching 11 responses to our label space, we get a really big boost in task accuracy for this problem just by using greedy decoding. However, can we do better if we don't miss matching those 11 responses? Let's use the activations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting (Activation Retrieval/Likelihood Estimation)\n",
    "\n",
    "For activation retrieval, we need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All LLaMA-2 models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed.\n",
    "\n",
    "If you are on the cluster, the tokenizer may be loaded from `/model-weights/Llama-2-7b-hf`. Otherwise, you'll need to download the `config.json`, `tokenizer.json`, `tokenizer.model`, and `tokenizer_config.json` from there to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 15043, 445, 338, 263, 1243]\n",
      "Decoded Tokens: <s> Hello this is a test\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"BOS\")\n",
    "encoded_tokens = tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = tokenizer.decode(encoded_tokens)\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1939,  694, 3869, 4874])\n",
      "No no Yes yes\n"
     ]
    }
   ],
   "source": [
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompts[0], [\"No\", \"no\", \"Yes\", \"yes\"])\n",
    "print(label_token_ids)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "print(tokenizer.decode([1939, 694, 3869, 4874]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the token ids of our labels to extract the probabilties from the vocabulary of the model. Note that we're actually assigning two tokens for each label (\"No\" and \"no\"), as the model might think one is more likely than the other in its probability distribution, but they are essentially the same answer.\n",
    "\n",
    "The token id corresponds to the index of the token in the vocabulary matrix of the underlying model. For a discussion and demonstration of how this extraction is done, see the [AGs News Notebook](src/reference_implementations/prompting_vector_llms/llm_prompting_examples/llm_prompt_ag_news.ipynb) and the comments in the `get_label_with_highest_likelihood` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:58<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "activation_map_to_label = {0: \"No\", 1: \"No\", 2: \"Yes\", 3: \"Yes\"}\n",
    "predicted_labels = []\n",
    "# For memory management, we split the prompts into batches of size 1, as activations are fairly heavy\n",
    "prompt_batches = split_prompts_into_batches(prompts, 1)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    # Note the configuration temperature won't matter for our setting, as we're not using sampling when\n",
    "    # extracting activations\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], greedy_generation_config)\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        # For each prompt we extract the activations and calculate which label had the high likelihood.\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, activation_map_to_label\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.82\n",
      "Confusion Matrix with ordering ['No', 'Yes']\n",
      "[[28 11]\n",
      " [ 7 54]]\n",
      "========================================================\n",
      "Label: No, F1: 0.7567567567567569, Precision: 0.8, Recall: 0.717948717948718\n",
      "Label: Yes, F1: 0.8571428571428572, Precision: 0.8307692307692308, Recall: 0.8852459016393442\n"
     ]
    }
   ],
   "source": [
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a small improvement in the performance of this task using activations, but, importantly, we are no longer failing to match 11 answers in the generation and using a random guess in those instances. This means that more of our answers are grounded in the models probability estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Zero-shot Prompting (Poor Label Space Choice)\n",
    "\n",
    "Because there are only two categories in our label space (yes/no) and the question is fairly \"leading.\" That is,  the model tends to answer in our label space through greedy decoding fairly well, (though there are still 11 instances where it doesn't). However, it's not always easy to select a label space that the model consistently produces an answer in for zero-shot (see [AG's News Notebook](src/reference_implementations/prompting_vector_llms/llm_prompting_examples/llm_prompt_ag_news.ipynb)). There are techniques that can be used to help this (like re-writing the prompt or using few-shot prompts), but what happens if we choose a bad label space?\n",
    "\n",
    "Let's try it out below. Instead of expecting a \"Yes\" or \"No\" answer, let's \"expect\" True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Example\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer (True or False):\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "lowercase_labels = [label.lower() for label in label_ordering]\n",
    "\n",
    "yes_no_prompts = []\n",
    "for test_title, test_passage, test_question in zip(bool_q_test_titles, bool_q_test_passages, bool_q_test_questions):\n",
    "    prompt = f\"{test_title} -- {test_passage}\\nquestion: {test_question}?\\nanswer (True or False):\"\n",
    "    yes_no_prompts.append(prompt)\n",
    "\n",
    "print(f\"Prompt Example\\n{yes_no_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7700, 2089, 5852, 1565])\n",
      "False false True true\n"
     ]
    }
   ],
   "source": [
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, yes_no_prompts[0], [\"False\", \"false\", \"True\", \"true\"])\n",
    "print(label_token_ids)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "print(tokenizer.decode([7700, 2089, 5852, 1565]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:44<00:00, 16.45s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "unmatched_predictions = []\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "prompt_batches = split_prompts_into_batches(yes_no_prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    responses = model.generate(prompt_batch, greedy_generation_config)\n",
    "    processed_responses = [generation.strip().lower() for generation in responses.generation[\"sequences\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in processed_responses:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            unmatched_predictions.append(potential_prediction)\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 Responses did not match the label space ['false', 'true']\n",
      "Some examples of unmatched responses: ['there', 'there', 'stephen', 'krist', 'pat', 'the', 'no', 'green', 'white', 'the']\n",
      "Prediction Accuracy: 0.48\n",
      "Confusion Matrix with ordering ['false', 'true']\n",
      "[[19 20]\n",
      " [32 29]]\n",
      "========================================================\n",
      "Label: false, F1: 0.42222222222222217, Precision: 0.37254901960784315, Recall: 0.48717948717948717\n",
      "Label: true, F1: 0.5272727272727273, Precision: 0.5918367346938775, Recall: 0.47540983606557374\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(unmatched_predictions)} Responses did not match the label space {lowercase_labels}\")\n",
    "print(f\"Some examples of unmatched responses: {sample(unmatched_predictions, 10)}\")\n",
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label].lower() for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=lowercase_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the responses don't match up well with our label space, which we expected based on our question phrasing. The question is whether a poorly chosen label space is essentially useless from a prediction standpoint? To help answer this question, let's consider the results when using activation extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting (Poor Label Choice Activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:15<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "activation_map_to_label = {0: \"False\", 1: \"False\", 2: \"True\", 3: \"True\"}\n",
    "predicted_labels = []\n",
    "# For memory management, we split the prompts into batches of size 1, as activations are fairly heavy\n",
    "prompt_batches = split_prompts_into_batches(yes_no_prompts, 1)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    # Note the configuration temperature won't matter for our setting, as we're not using sampling when\n",
    "    # extracting activations\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], greedy_generation_config)\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        # For each prompt we extract the activations and calculate which label had the high likelihood.\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, activation_map_to_label\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.58\n",
      "Confusion Matrix with ordering ['False', 'True']\n",
      "[[22 17]\n",
      " [25 36]]\n",
      "========================================================\n",
      "Label: False, F1: 0.5116279069767442, Precision: 0.46808510638297873, Recall: 0.5641025641025641\n",
      "Label: True, F1: 0.631578947368421, Precision: 0.6792452830188679, Recall: 0.5901639344262295\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy is still not great, we get an improvement using activations over pure generation and the results suggest that there is still some information encoded in the True/False answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=2 Few-Shot Examples (Greedy Decoding)\n",
    "\n",
    "In this example, we consider the affect of adding demonstrations to our prompts. That is, for this problem, does adding a small number of demonstrationss improve accuracy over zero-shot prompting?\n",
    "\n",
    "**NOTE**: Some of the Bool Q examples are quite long. Our LLaMA-2 implementation limits input lengths to 512. So sometimes adding two or even just one demonstration puts us over this limit. The `create_prompts` function handles this by putting as many demonstrations \"as possible\" (up to 2) into our prompts. In many of the few-shot prompting examples, the benefit from demonstrations scales with the number of demonstrations provided. So we may not get an increase in task accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt is too long: 534. Using 1-shot prompt\n",
      "Prompt is too long: 593. Using 0-shot prompt\n",
      "Prompt is too long: 517. Using 1-shot prompt\n",
      "Prompt is too long: 557. Using 1-shot prompt\n",
      "Prompt is too long: 552. Using 1-shot prompt\n",
      "Prompt is too long: 548. Using 1-shot prompt\n",
      "Prompt is too long: 598. Using 1-shot prompt\n"
     ]
    }
   ],
   "source": [
    "label_map_two = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering_two = [\"No\", \"Yes\"]\n",
    "lowercase_labels = [label.lower() for label in label_ordering_two]\n",
    "demonstrations_two = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, label_map_two, 2\n",
    ")\n",
    "prompts_two = create_prompts(\n",
    "    demonstrations_two, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check (chess) -- In friendly games, the checking player customarily says ``check'' when making a checking move. Announcing ``check'' is not required under the rules of chess and it is usually not done in formal games. Until the early 20th century a player was expected to announce ``check'', and some sources of rules even required it (Hooper & Whyld 1992:74).\n",
      "question: do we have to say check in chess?\n",
      "answer: No\n",
      "\n",
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts_two[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:16<00:00, 13.65s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "unmatched_predictions = []\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "prompt_batches = split_prompts_into_batches(prompts_two, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    responses = model.generate(prompt_batch, greedy_generation_config)\n",
    "    processed_responses = [generation.strip().lower() for generation in responses.generation[\"sequences\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in processed_responses:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            unmatched_predictions.append(potential_prediction)\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Responses did not match the label space ['no', 'yes']\n",
      "Some examples of unmatched responses: []\n",
      "Prediction Accuracy: 0.83\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[27 12]\n",
      " [ 5 56]]\n",
      "========================================================\n",
      "Label: no, F1: 0.7605633802816902, Precision: 0.84375, Recall: 0.6923076923076923\n",
      "Label: yes, F1: 0.868217054263566, Precision: 0.8235294117647058, Recall: 0.9180327868852459\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(unmatched_predictions)} Responses did not match the label space {lowercase_labels}\")\n",
    "print(f\"Some examples of unmatched responses: {unmatched_predictions}\")\n",
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map_two[label].lower() for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=lowercase_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we don't see a significant increase in accuracy for this task with 2-shot (at most) prompts. However, one immediate benefit that can be seen is that the number of responses that we were unable to map to our label space drops from 11 -> 0, which is a useful result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
