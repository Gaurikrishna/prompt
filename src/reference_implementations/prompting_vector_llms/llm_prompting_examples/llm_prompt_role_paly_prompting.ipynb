{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchongzhang/Projects/PromptEngineeringLaboratory/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "from utils import split_prompts_into_batches\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with role-play prompting, as detailed in the paper \"Better Zero-Shot Reasoning with Role-Play Prompting\" (https://arxiv.org/pdf/2308.07702.pdf). The idea is fairly simple. Suppose we want the language model to perform a certain task T. If we \"immerse\" the model into the role of an expert on task T (or a role that is closely related to task T) through some conversational prompts, then the model might perform task T better. \n",
    "\n",
    "Following (roughly) the paper, we will compare role-play prompting with two other methods: Zero-shot prompting and Zero-shot CoT on the task of solving math word problems using the MultiArith dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'e5ee7525-cc82-46f1-b7d0-aa25c3994571',\n",
       "  'name': 'llama2-70b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the LLama 70B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama2-70b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 60, \"top_k\": 4, \"top_p\": 1.0, \"temperature\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiArith: Math Word Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiArith is a dataset consisting of math word problems. The following function is used to read the data from the raw json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(dataset_path: str) -> Tuple[List[str], List[str]]:\n",
    "    questions = []\n",
    "    answers = []\n",
    "    with open(dataset_path) as f:\n",
    "        json_data = json.load(f)\n",
    "        for line in json_data:\n",
    "            q = line[\"sQuestion\"].strip()\n",
    "            a = str(line[\"lSolutions\"][0])\n",
    "            if a[-2:] == \".0\":\n",
    "                a = a[:-2]\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example question and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For Halloween Debby and her sister combined the candy they received. Debby had 32 pieces of candy while her sister had 42. If they ate 35 pieces the first night, how many pieces do they have left?\n",
      "Correct answer: 39\n"
     ]
    }
   ],
   "source": [
    "questions, answers = data_reader(\"resources/multi_arith_dataset/MultiArith.json\")\n",
    "\n",
    "print(f\"Question: {questions[0]}\")\n",
    "print(f\"Correct answer: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly sample a subset and use it for evaluation since the whole dataset is quite large and prompting takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "sample_questions, sample_answers = zip(*random.sample(list(zip(questions, answers)), num_samples))\n",
    "sample_questions = list(sample_questions)\n",
    "sample_answers = [int(answer) for answer in list(sample_answers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets for MultiArith are integers, and since the model would likely produce answers that are not integers, we need a way to extract a numerical answer from the model's response string. We perform this by another round of prompting. More precisely, for each question, after getting the answer generated by the LLM, we concatenate the question, answer, and an answer trigger together and feed them into the model again. \n",
    "\n",
    "For MultiArith specifically, we use the answer trigger \"Therefore, the answer (arabic numerals) is\" and we extract the first integer in the model's response to the second prompt as its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numerical_answer(response_str) -> int:\n",
    "    response_str = response_str.replace(\",\", \"\")\n",
    "    response_str = response_str.replace(\".\", \"\")\n",
    "    numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', response_str)]\n",
    "    if len(numbers) > 0:\n",
    "        return int(numbers[0])\n",
    "    else:\n",
    "        # if the model reponse does not contain any number, we just return a random integer.\n",
    "        return random.randint(3000, 3100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try Zero-shot prompting with no CoT. We will begin by storing all the model's responses to the raw math questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:34<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_responses = []\n",
    "for sample_question in tqdm(sample_questions):\n",
    "    generation = model.generate(sample_question, long_generation_config)\n",
    "    zero_shot_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ An example question and the model response ============\n",
      "Question: Katie's team won their dodgeball game and scored 12 points total. If Katie scored 4 of the points and everyone else scored 4 points each, how many players were on her team?\n",
      "Correct answer: 2\n",
      "Model response:\n",
      "There were 4 players on Katie's team.\n",
      "Katie's team won their dodgeball game and scored 12 points total. If Katie scored 4 of the points and everyone else scored 4 points each, how many players were on her team? There\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(\"============ An example question and the model response ============\")\n",
    "print(f\"Question: {sample_questions[i]}\")\n",
    "print(f\"Correct answer: {sample_answers[i]}\")\n",
    "print(f\"Model response:{zero_shot_responses[i]['sequences'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate each one of the model's responses with the answer trigger and feed the result into the model again in order to extract numerical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_answer_trigger = \"Therefore, the answer (arabic numerals) is\"\n",
    "multi_arith_concatenated_prompts = []\n",
    "second_generations = []\n",
    "for sample_question, model_response in zip(sample_questions, zero_shot_responses):\n",
    "    response_str = model_response['sequences'][0]\n",
    "    concatenated_prompt = f\"{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ An example question and the model response to the concatenated prompt ============\n",
      "Question: Katie's team won their dodgeball game and scored 12 points total. If Katie scored 4 of the points and everyone else scored 4 points each, how many players were on her team?\n",
      "Correct answer: 2\n",
      "Second round prompt: Katie's team won their dodgeball game and scored 12 points total. If Katie scored 4 of the points and everyone else scored 4 points each, how many players were on her team?'\n",
      "'\n",
      "There were 4 players on Katie's team.\n",
      "Katie's team won their dodgeball game and scored 12 points total. If Katie scored 4 of the points and everyone else scored 4 points each, how many players were on her team? There'\n",
      "'Therefore, the answer (arabic numerals) is\n",
      "Model response:: 4\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"============ An example question and the model response to the concatenated prompt ============\")\n",
    "print(f\"Question: {sample_questions[i]}\")\n",
    "print(f\"Correct answer: {sample_answers[i]}\")\n",
    "print(f\"Second round prompt: {multi_arith_concatenated_prompts[i]}\")\n",
    "print(f\"Model response:{second_generations[i]['sequences'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the extracted numerical answers with the correct answers to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_answers = [extract_numerical_answer(second_generation['sequences'][0]) for second_generation in second_generations]\n",
    "accuracy = np.sum(np.array(final_answers) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is not very good. Next, we will follow the same evaluation procedure to test the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try Zero-shot CoT as another baseline for comparison. This is pretty similar to the vanilla zero-shot approach above, except we append the sentence \"Let's think step by step\" to the end of each prompt to encourage the model to perform CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_post_prompt = \"Let's think step by step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:07<00:00, 13.35s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_cot_responses = []\n",
    "for sample_question in tqdm(sample_questions):\n",
    "    prompt = f\"{sample_question}\\n{cot_post_prompt}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    zero_shot_cot_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_cot = []\n",
    "second_generations_cot = []\n",
    "for sample_question, model_response in zip(sample_questions, zero_shot_cot_responses):\n",
    "    response_str = model_response['sequences'][0]\n",
    "    concatenated_prompt = f\"{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_cot.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_cot.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_answers_cot = [extract_numerical_answer(second_generation['sequences'][0]) for second_generation in second_generations_cot]\n",
    "accuracy = np.sum(np.array(final_answers_cot) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 3046, 4, 15, 3054, 11, 20, 12, 3055, 4, 3002, 55, 32, 12, 45, 43, 3032, 10, 35, 27, 1, 6, 16, 60, 48, 19, 36, 64, 8, 34, 41, 9, 60, 8, 27, 29, 6, 63, 6, 36, 11, 33, 3051, 49, 86, 6, 5, 3035, 73]\n",
      "[4, 45, 3, 4, 15, 4, 11, 4, 36, 9, 2, 64, 51, 32, 9, 45, 43, 18, 10, 35, 27, 5, 6, 16, 58, 48, 49, 36, 80, 8, 34, 41, 9, 60, 8, 21, 29, 6, 63, 12, 21, 6, 27, 4, 63, 86, 6, 2, 3, 73]\n"
     ]
    }
   ],
   "source": [
    "print(final_answers_cot)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adding the simple sentence \"Let's think step by step.\", CoT induces a significant improvement in performance. Now let's see whether role-play prompting works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role-play Prompting\n",
    "\n",
    "We consider three different role-play prompts, representing three different levels of immersion. \n",
    "\n",
    "In the first level, we simply inform the model of the role it is expected to play before asking the question.\n",
    "\n",
    "In the second level, immersion is enhanced by adding complementary descriptions of the role in the prompt.\n",
    "\n",
    "In the third level, we append to the end of the level-2 prompt a \"response\" in which the model acknowledges the role it plays. Because we are using Llama-70b rather than Llamo-70b-chat, it is tricky to induce this kind of response from the model. So instead, we just artificially create the response and concatenate it with the level-2 prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt_level1 = \"From now on, you are a math teacher. Please answer the following question\"\n",
    "role_prompt_level2 = \"From now on, you are an excellent math teacher and always teach your students math problems correctly. I am one of your students and ask you the following question.\"\n",
    "role_prompt_level3 = \"From now on, you are an excellent math teacher and always teach your students math problems correctly. And I am one of your students. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:17<00:00, 14.75s/it]\n"
     ]
    }
   ],
   "source": [
    "level1_responses = []\n",
    "for sample_question in tqdm(sample_questions):\n",
    "    prompt = f\"{role_prompt_level1}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level1_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level1 = []\n",
    "second_generations_level1 = []\n",
    "for sample_question, model_response in zip(sample_questions, level1_responses):\n",
    "    response_str = model_response['sequences'][0]\n",
    "    concatenated_prompt = f\"{role_prompt_level1}\\n{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_level1.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level1.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_answers_level1 = [extract_numerical_answer(second_generation['sequences'][0]) for second_generation in second_generations_level1]\n",
    "accuracy = np.sum(np.array(final_answers_level1) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply telling the model the role we wish it to play does not seem to help. We can try printing out some of the model's responses to our role-play prompt to see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A. 10 books\n",
      "B. 13 books\n",
      "C. 14 books\n",
      "D. 17 books\n",
      "E. 21 books\n",
      "I would say C, but I'm not a math teacher.\n",
      "C, but I'm not a math\n",
      "[6, 45, 4, 11, 13, 3004, 14, 6, 100, 7, 3, 16, 5, 32, 16, 85, 46, 18, 11, 3072, 27, 6, 30, 11, 3042, 48, 3079, 18, 80, 13, 20, 14, 27, 23, 18, 12, 29, 5, 9, 15, 36, 3, 33, 3058, 3087, 90, 10, 3, 4, 43]\n",
      "[4, 45, 3, 4, 15, 4, 11, 4, 36, 9, 2, 64, 51, 32, 9, 45, 43, 18, 10, 35, 27, 5, 6, 16, 58, 48, 49, 36, 80, 8, 34, 41, 9, 60, 8, 21, 29, 6, 63, 12, 21, 6, 27, 4, 63, 86, 6, 2, 3, 73]\n"
     ]
    }
   ],
   "source": [
    "print(level1_responses[6]['sequences'][0])\n",
    "print(final_answers_level1)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:25<00:00, 14.92s/it]\n"
     ]
    }
   ],
   "source": [
    "level2_responses = []\n",
    "for sample_question in tqdm(sample_questions):\n",
    "    prompt = f\"{role_prompt_level2}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level2_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level2 = []\n",
    "second_generations_level2 = []\n",
    "for sample_question, model_response in zip(sample_questions, level2_responses):\n",
    "    response_str = model_response['sequences'][0]\n",
    "    concatenated_prompt = f\"{role_prompt_level2}\\n{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_level2.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level2.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n"
     ]
    }
   ],
   "source": [
    "final_answers_level2 = [extract_numerical_answer(second_generation['sequences'][0]) for second_generation in second_generations_level2]\n",
    "accuracy = np.sum(np.array(final_answers_level2) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that adding more detailed descriptions to the role slightly increases the performance. But the improvement is very negligible: from level 1 to level 2, the accuracy increased from 0.14 to 0.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am sure you would tell me that there are 11 books in the bin.\n",
      "But what if I ask you the following question.\n",
      "A book store had 20 books in the bargin bin. If they sold 10 books, but then put 10 more\n",
      "[14, 54, 3036, 3073, 5, 4, 10, 6, 6, 3029, 3099, 7, 48, 3052, 11, 17, 46, 3037, 40, 3037, 34, 9, 3, 16, 37, 6, 65, 36, 80, 16, 30, 1, 13, 3005, 2, 18, 29, 6, 36, 3, 3031, 3025, 28, 3094, 98, 86, 6, 3083, 3125, 75]\n",
      "[4, 45, 3, 4, 15, 4, 11, 4, 36, 9, 2, 64, 51, 32, 9, 45, 43, 18, 10, 35, 27, 5, 6, 16, 58, 48, 49, 36, 80, 8, 34, 41, 9, 60, 8, 21, 29, 6, 63, 12, 21, 6, 27, 4, 63, 86, 6, 2, 3, 73]\n"
     ]
    }
   ],
   "source": [
    "print(level2_responses[6]['sequences'][0])\n",
    "print(final_answers_level2)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how well level-3 role-paly prompting work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_after_last_period(input_string: str) -> str:\n",
    "    # Find the last period in the string\n",
    "    last_period_index = input_string.rfind('.')\n",
    "\n",
    "    # If a period is found, discard everything after it\n",
    "    if last_period_index != -1:\n",
    "        result_string = input_string[:last_period_index + 1]\n",
    "    else:\n",
    "        # If no period is found, return the original string\n",
    "        result_string = input_string\n",
    "\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the artificial response we will append to the role prompt. As mentioned before, it is tricky to induce this kind of response from the model. We print out an example of the mode's actual response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt_level3_follow_up = \"That’s great to hear! As your math teacher, I’ll do my best to explain mathematical concepts correctly so that you can understand them easily. Feel free to ask any math problems or questions you have, and I’ll be glad to assist you. Let’s dive into the world of mathematics and explore its wonders together!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀\n",
      "It’s so nice to be your student.\n",
      "I’m so happy that you are my student.\n",
      "I’m so happy that you are my student. 😀\n",
      "I’m so happy that you are my student. 😀 😀\n",
      "I’m so happy that you are my student. 😀 😀 😀\n",
      "I’m so happy that\n"
     ]
    }
   ],
   "source": [
    "generation_level3_role_prompt = model.generate(role_prompt_level3, long_generation_config)\n",
    "print(generation_level3_role_prompt.generation['sequences'][0])\n",
    "role_prompt_response_level3 = discard_after_last_period(generation_level3_role_prompt.generation['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [19:57<00:00, 23.96s/it]\n"
     ]
    }
   ],
   "source": [
    "level3_responses = []\n",
    "for sample_question in tqdm(sample_questions):\n",
    "    prompt = f\"{role_prompt_level3}\\n{role_prompt_level3_follow_up}\\n{sample_question}\"\n",
    "    generation = model.generate(prompt, long_generation_config)\n",
    "    level3_responses.append(generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_arith_concatenated_prompts_level3 = []\n",
    "second_generations_level3 = []\n",
    "for sample_question, model_response in zip(sample_questions, level3_responses):\n",
    "    response_str = model_response['sequences'][0]\n",
    "    concatenated_prompt = f\"{role_prompt_level3}\\n{role_prompt_level3_follow_up}\\n{sample_question}'\\n'{response_str}'\\n'{multi_arith_answer_trigger}\"\n",
    "    multi_arith_concatenated_prompts_level3.append(concatenated_prompt)\n",
    "    second_generation = model.generate(concatenated_prompt, long_generation_config)\n",
    "    second_generations_level3.append(second_generation.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42\n"
     ]
    }
   ],
   "source": [
    "final_answers_level3 = [extract_numerical_answer(second_generation['sequences'][0]) for second_generation in second_generations_level3]\n",
    "accuracy = np.sum(np.array(final_answers_level3) == np.array(sample_answers)) / len(sample_answers)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the fake response in which the model acknowledges its role improves the performance quite markedly. We got better accuracy than Zero-shot or the previous two immersion levels. But the accuracy is still worse than Zero-shot CoT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11'\n",
      "'Therefore, the answer (roman numerals) is XI'\n",
      "'Therefore, the answer (greek numerals) is ΙΙ'\n",
      "'Therefore, the answer (hebrew numerals) is א'\n",
      "'Therefore, the answer (binary) is 1011'\n",
      "'Therefore, the answer (octal) is 13'\n",
      "'Therefore, the answer (hexadec\n",
      "[5, 9, 3, 4, 15, 4, 11, 4, 48, 15, 7, 7, 53, 32, 9, 65, 43, -8, 1, 35, 13, 3, 1, 16, 58, 16, 49, 3031, 80, 3079, 11, 31, 11, 60, 24, 27, 34, 6, 9, 12, 21, 5, 90, 4, 9, 86, 5, 2, 4, 43]\n",
      "[4, 45, 3, 4, 15, 4, 11, 4, 36, 9, 2, 64, 51, 32, 9, 45, 43, 18, 10, 35, 27, 5, 6, 16, 58, 48, 49, 36, 80, 8, 34, 41, 9, 60, 8, 21, 29, 6, 63, 12, 21, 6, 27, 4, 63, 86, 6, 2, 3, 73]\n"
     ]
    }
   ],
   "source": [
    "print(second_generations_level3[6]['sequences'][0])\n",
    "print(final_answers_level3)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Concluding Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper provided some evidence that role-play prompting can work better than CoT in many tasks, but in their case the model was able to acknowledge its role on its own, so there was no need to artifically create a fake response. This could be the reason why role-play prompting does not work as well as CoT in our case. Intuitively, one might say that if the model acknowledges its role on its own, then the responses which come after that naturally extend this context, so in some sense it is similar to CoT. \n",
    "\n",
    "We can also observe that in level 1 and 2, the model gave some incoherent responses which suggest it did not immerse itself in the role of a math teacher. This could be the reason why these two levels failed to improve upon Zero-shot. The model we are using here (Llama-70b) has not been fine-tuned for chat purposes, so it is harder to induce conversation-like behaviour from the model, which can be important for role-play prompting to work. A natural experiment one might want to run to verify this idea is to use a model that has been fine-tuned for chat purposes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
