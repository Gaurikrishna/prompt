{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f91f0505770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import kscope\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "# Setting seed to ensure reproducibility\n",
    "torch.manual_seed(1776)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll perform a simple experiment. The question is:\n",
    "\n",
    "_Do the discrete prompts learned to optimize T5s ability to perform the SST2 sentiment analysis task also improve performance for LLaMA-2-7B?_\n",
    "\n",
    "__NOTE__: We performed discrete prompt optimization using an in-house implementation of AutoPrompt. While the implementation is not included in this iteration of the lab, if you are interested feel free to ask your facilitator about it and we can provide access to a repository containing the implementation.\n",
    "\n",
    "In the optimization, the original seed prompt was \n",
    "\n",
    "    \"Generate the sentiment of the next sentence. \". \n",
    "\n",
    "For T5, this prompt induced about 68% accuracy on the whole SST2 dataset, of which we consider a sample to perform the transferrability measurements conducted here. After gradient based search optimization, we produced the optimized prompt \n",
    "\n",
    "    \"tumour negative .05. Positive respins the Contains sentence. \" \n",
    "    \n",
    "which yielded an accuracy of about 81%. If the optimization was run for additional iterations, we got a final prompt of\n",
    "\n",
    "    \"childcare negative .05. Positive respins wSt Thank sentence.\" \n",
    "    \n",
    "with an accuracy of 83%. Both of these prompts are difficult to read and it's not clear why these induce improved performance for T5, beyond the fact that they do incorporate the label space (Negative, Positive).\n",
    "\n",
    "Let's determine if either of these odd but apparently performant prompts will improve also results for LLaMA-2-7B over the original as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"Generate the sentiment of the next sentence. \"\n",
    "optimized_prompt_1 = \"tumour negative .05. Positive respins the Contains sentence. \"\n",
    "optimized_prompt_2 = \"childcare negative .05. Positive respins wSt Thank sentence. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll setup the tokenizers associated with the target language models.\n",
    "\n",
    "For activation retrieval, we need to instantiate a tokenizer to obtain appropriate token indices for our labels.\n",
    "\n",
    "__NOTE__: All LLaMA-2 models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed.\n",
    "\n",
    "If you are on the cluster, the tokenizer may be loaded from `/model-weights/Llama-2-7b-hf`. Otherwise, you'll need to download the `config.json`, `tokenizer.json`, `tokenizer.model`, and `tokenizer_config.json` from there to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-2 Encoded Tokens: [1, 15043, 445, 338, 263, 1243]\n",
      "T5 Encoded Tokens: [8774, 48, 19, 3, 9, 794, 1]\n",
      "LLaMA Decoded Tokens: <s> Hello this is a test\n",
      "T5 Decoded Tokens: Hello this is a test</s>\n"
     ]
    }
   ],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"/Users/david/Desktop/LLaMA2_Tokenizer\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-large-lm-adapt\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"<s>\")\n",
    "llama_encoded_tokens = llama_tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"LLaMA-2 Encoded Tokens: {llama_encoded_tokens}\")\n",
    "\n",
    "t5_encoded_tokens = t5_tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"T5 Encoded Tokens: {t5_encoded_tokens}\")\n",
    "\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "llama_decoded_tokens = llama_tokenizer.decode(llama_encoded_tokens)\n",
    "print(f\"LLaMA Decoded Tokens: {llama_decoded_tokens}\")\n",
    "\n",
    "t5_decoded_tokens = t5_tokenizer.decode(t5_encoded_tokens)\n",
    "print(f\"T5 Decoded Tokens: {t5_decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup some of the necessary variables for the inference below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Device cpu\n"
     ]
    }
   ],
   "source": [
    "label_words = [\"negative\", \"positive\"]\n",
    "# How big should inference batches be\n",
    "batch_size = 10\n",
    "# How many batches in total to process from the dataloader (batch_size*batches_to_sample = datapoints to process)\n",
    "batches_to_sample = 20\n",
    "\n",
    "# Determine whether cuda and a GPU are available to speed up processing\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Detected Device {device}\")\n",
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': \"it 's a charming and often affecting journey . \",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\", split=\"validation\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate answer extraction, we need to know the tokenization ids for each of the labels we expect the model to response with. In this case, we optimized the prompts for T5 to produce either \"negative\" or \"positive.\" So we use the tokenizers to extract the IDs associated with these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3251, 403, 278, 19688, 310, 278, 2446, 10541, 29889, 306, 5360, 445, 14064, 29991, 8178]\n",
      "[1, 3251, 403, 278, 19688, 310, 278, 2446, 10541, 29889, 306, 5360, 445, 14064, 29991, 6374]\n",
      "[6939, 2206, 8, 6493, 13, 8, 416, 7142, 5, 27, 333, 48, 1974, 55, 2841, 1]\n",
      "[6939, 2206, 8, 6493, 13, 8, 416, 7142, 5, 27, 333, 48, 1974, 55, 1465, 1]\n"
     ]
    }
   ],
   "source": [
    "dummy_sentence = \"I love this movie!\"\n",
    "print(llama_tokenizer(f\"{initial_prompt}{dummy_sentence} negative\")[\"input_ids\"])\n",
    "print(llama_tokenizer(f\"{initial_prompt}{dummy_sentence} positive\")[\"input_ids\"])\n",
    "print(t5_tokenizer(f\"{initial_prompt}{dummy_sentence} negative </s>\", add_special_tokens=False)[\"input_ids\"])\n",
    "print(t5_tokenizer(f\"{initial_prompt}{dummy_sentence} positive </s>\", add_special_tokens=False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the LLaMA-2 and T5 Tokenizers output only differs in the last token ID (ignoring the end of sentence token ID of 1). So we infer that this must be the token associated with negative and positive labels. Let's confirm this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Decode: negative positive\n",
      "T5 Decode: negative positive\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLaMA Decode: {llama_tokenizer.decode([8178, 6374])}\")\n",
    "print(f\"T5 Decode: {t5_tokenizer.decode([2841, 1465])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_label_tokens = {\"negative\": [8178], \"positive\": [6374]}\n",
    "t5_label_tokens = {\"negative\": [2841], \"positive\": [1465]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by measuring the performance of LLaMA-2 7B with the initial prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='llm.cluster.local', port=3001): Max retries exceeded with url: /models/instances (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa918c35070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fa918c35070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='llm.cluster.local', port=3001): Max retries exceeded with url: /models/instances (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa918c35070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m kscope\u001b[38;5;241m.\u001b[39mClient(gateway_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm.cluster.local\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_instances\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/kaleidoscope_sdk.py:87\u001b[0m, in \u001b[0;36mClient.model_instances\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_instances\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of available model instances\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/kaleidoscope_sdk.py:142\u001b[0m, in \u001b[0;36mGatewaySession.get_model_instances\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_instances\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_addr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/utils.py:52\u001b[0m, in \u001b[0;36mget\u001b[0;34m(addr, auth_key, headers)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth_key:\n\u001b[1;32m     50\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauth_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m check_response(resp)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='llm.cluster.local', port=3001): Max retries exceeded with url: /models/instances (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa918c35070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))"
     ]
    }
   ],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)\n",
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires an generation configuration to run, but we're going to be extracting the label likelihoods rather than generating. So the only setting we really care about is `max_tokens = 1`. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)\n",
    "\n",
    "generation_config = {\"max_tokens\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_str(instruction: str, sentences: List[str]) -> List[str]:\n",
    "    return [f\"{instruction}{sentence}\" for sentence in sentences]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the prompts we're creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate the sentiment of the next sentence. the son 's room is a triumph of gentility that earns its moments of pathos . \n",
      "tumour negative .05. Positive respins the Contains sentence. the son 's room is a triumph of gentility that earns its moments of pathos . \n",
      "childcare negative .05. Positive respins wSt Thank sentence. the son 's room is a triumph of gentility that earns its moments of pathos . \n"
     ]
    }
   ],
   "source": [
    "example_batch = next(iter(dataloader))\n",
    "example_prompts_initial = create_prompt_str(initial_prompt, example_batch[\"sentence\"])\n",
    "example_prompts_optimized_1 = create_prompt_str(optimized_prompt_1, example_batch[\"sentence\"])\n",
    "example_prompts_optimized_2 = create_prompt_str(optimized_prompt_2, example_batch[\"sentence\"])\n",
    "print(example_prompts_initial[0])\n",
    "print(example_prompts_optimized_1[0])\n",
    "print(example_prompts_optimized_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_label_from_activations_llama(label_token_ids: torch.Tensor, layer_matrix: torch.Tensor) -> int:\n",
    "    # The activations we care about are the last token (corresponding to our label token) and the values for our label\n",
    "    # vocabulary\n",
    "    label_activations = layer_matrix[-1][label_token_ids].float()\n",
    "    # Softmax is not strictly necessary, but it helps to contextualize the \"probability\" the model associates with each\n",
    "    # label relative to the others\n",
    "    label_distributions = softmax(label_activations)\n",
    "    # We select the label index with the largest value\n",
    "    max_label_index = torch.argmax(label_distributions)\n",
    "    return max_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "report: List[Tuple[str, float]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([llama_label_tokens[\"negative\"], llama_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(initial_prompt, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    assert len(prompts) == len(labels)\n",
    "    # Because activation retrieval is a somewhat heavy operation, we just do batches of 1\n",
    "    for prompt, label in zip(prompts, labels):\n",
    "        activations = model.get_activations([prompt], [last_layer_name], generation_config)\n",
    "        last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "        predicted_label = select_label_from_activations_llama(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((initial_prompt, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try both of our \"optimized prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([llama_label_tokens[\"negative\"], llama_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(optimized_prompt_1, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    assert len(prompts) == len(labels)\n",
    "    # Because activation retrieval is a somewhat heavy operation, we just do batches of 1\n",
    "    for prompt, label in zip(prompts, labels):\n",
    "        activations = model.get_activations([prompt], [last_layer_name], generation_config)\n",
    "        last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "        predicted_label = select_label_from_activations_llama(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([llama_label_tokens[\"negative\"], llama_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(optimized_prompt_2, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    assert len(prompts) == len(labels)\n",
    "    # Because activation retrieval is a somewhat heavy operation, we just do batches of 1\n",
    "    for prompt, label in zip(prompts, labels):\n",
    "        activations = model.get_activations([prompt], [last_layer_name], generation_config)\n",
    "        last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "        predicted_label = select_label_from_activations_llama(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_2, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.615\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HugggingFace T5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these prompts in the context of the original T5 model from HuggingFace to verify that they indeed do \"optimize\" the prompt performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and set it to eval mode\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/t5-large-lm-adapt\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_decoder_inputs(\n",
    "    prompts: List[str], t5_tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast], device: str\n",
    ") -> Tuple[BatchEncoding, BatchEncoding]:\n",
    "    # Repeat each prompt twice (once for each label)\n",
    "    repeated_prompts = [prompt for prompt in prompts for i in range(2)]\n",
    "    # repeat label words for each repeated prompt\n",
    "    decoder_labels = [f\"{label_word} </s>\" for label_word in label_words] * len(prompts)\n",
    "    encoder_inputs = t5_tokenizer(\n",
    "        repeated_prompts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    decoder_inputs = t5_tokenizer(\n",
    "        decoder_labels,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=16,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    return encoder_inputs, decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihoods_from_t5_output(\n",
    "    output: Seq2SeqLMOutput, loss_func: torch.nn.CrossEntropyLoss, decoder_ids: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "    # Negative of the loss to get back to raw log-probabilities\n",
    "    log_likelihoods = -loss_func(output.logits.view(-1, output.logits.size(-1)), decoder_ids.view(-1))\n",
    "    batch_size, sequence_length, _ = output.logits.size()\n",
    "    # compute per-token log probability in a sequence.\n",
    "    # log_p has log probabilities for the following target output: [pos, it, ive]\n",
    "    log_likelihoods = log_likelihoods.view(batch_size, sequence_length)\n",
    "    # pad tokens have index -100 in HuggingFace.\n",
    "    good_log_p = log_likelihoods.masked_fill_(decoder_ids == -100, 0.0)\n",
    "    # good_log_p now has the log probability of the output sequence tokens.\n",
    "    # sum over the sequence length to compute the log probability for a full sequence.\n",
    "    return torch.sum(good_log_p, dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t5_model_on_encodings(\n",
    "    encoder_encodings: BatchEncoding, decoder_encodings: BatchEncoding, t5_model: T5ForConditionalGeneration\n",
    ") -> Tuple[Seq2SeqLMOutput, torch.Tensor]:\n",
    "    decoder_ids = decoder_encodings.input_ids\n",
    "    # we have to make sure that the PAD token is ignored.\n",
    "    # HuggingFace ignores a pad token if the token is -100!\n",
    "    decoder_ids = decoder_ids.masked_fill(decoder_ids == t5_tokenizer.pad_token_id, -100)\n",
    "    # Disable gradient tracking for faster inference\n",
    "    with torch.no_grad():\n",
    "        model_output = t5_model(\n",
    "            input_ids=encoder_encodings.input_ids,\n",
    "            attention_mask=encoder_encodings.attention_mask,\n",
    "            decoder_attention_mask=decoder_encodings.attention_mask,\n",
    "            decoder_input_ids=t5_model._shift_right(decoder_ids),\n",
    "            labels=None,\n",
    "        )\n",
    "    return model_output, decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_likelihoods(softmax_func: nn.Softmax, likelihoods: torch.Tensor) -> torch.tensor:\n",
    "    # Pair the likelihoods associated with negative and positive labels for each prompt\n",
    "    likelihoods = likelihoods.reshape(-1, 2)\n",
    "    likelihoods = softmax_func(likelihoods)\n",
    "    return torch.argmax(likelihoods, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we run the original prompt through the T5 Model to establish the \"baseline\" accuracy that we're hoping the optimized prompts improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m---> 22\u001b[0m \u001b[43mreport\u001b[49m\u001b[38;5;241m.\u001b[39mappend((initial_prompt, accuracy))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'report' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilities of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(initial_prompt, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_output(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "\n",
    "report.append((initial_prompt, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6800000071525574\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the first optimized prompt through T5 to see if there is a better accuracy outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     19\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m---> 20\u001b[0m \u001b[43mreport\u001b[49m\u001b[38;5;241m.\u001b[39mappend((optimized_prompt_1, accuracy))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'report' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilities of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(optimized_prompt_1, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_output(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the second optimized prompt to establish its performance on a small subset of the SST2 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     19\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m---> 20\u001b[0m \u001b[43mreport\u001b[49m\u001b[38;5;241m.\u001b[39mappend((optimized_prompt_2, accuracy))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'report' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilties of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(optimized_prompt_2, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_output(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_2, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599999904632568\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, let's summarize our findings and analyze the results that we've collected in the report dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "OPT Performance:\n",
      "Prompt: Generate the sentiment of the next sentence. , Accuracy: 0.6\n",
      "Prompt: tumour negative .05. Positive respins the Contains sentence. , Accuracy: 0.555\n",
      "Prompt: childcare negative .05. Positive respins wSt Thank sentence. , Accuracy: 0.615\n",
      "T5 Performance:\n",
      "Prompt: Generate the sentiment of the next sentence. , Accuracy: 0.6549999713897705\n",
      "Prompt: tumour negative .05. Positive respins the Contains sentence. , Accuracy: 0.7900000214576721\n",
      "Prompt: childcare negative .05. Positive respins wSt Thank sentence. , Accuracy: 0.7549999952316284\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary\")\n",
    "print(\"LLaMA-2 7B Performance:\")\n",
    "for prompt, acc in report[0:3]:\n",
    "    print(f\"Prompt: {prompt}, Accuracy: {acc}\")\n",
    "print(\"T5 Performance:\")\n",
    "for prompt, acc in report[3:6]:\n",
    "    print(f\"Prompt: {prompt}, Accuracy: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fairly clear that OPT does not do well with this type of prompt, whereas T5 does a pretty good job with this instruction prompt.\n",
    "\n",
    "The amazing part is that these weird prompts seem to improve the performance of T5, but also possibly the performance of OPT a little bit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
