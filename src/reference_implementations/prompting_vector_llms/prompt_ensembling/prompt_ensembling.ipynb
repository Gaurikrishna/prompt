{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from random import choice, sample\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from evaluate import EvaluationModule\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_mc_prompt,\n",
    "    create_second_prompt,\n",
    "    split_prompts_into_batches,\n",
    ")\n",
    "\n",
    "random.seed(2024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a client connection to the Kaleidoscope service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'efd66405-d0ef-48d0-8ca0-2eb0c7c8127f',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'e64b3690-96c3-4a50-b95f-adcb85e2a42c',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while llama_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while falcon_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vote Ensembling\n",
    "\n",
    "In the [CoPA Prompt Examples](copa_prompting_examples.ipynb) notebook, we looked at a few different ways to get the LLMs to perform the CoPA task. The best way was to evaluate the log probabilities of the candidates as completions, but we also considered a purely generative approach and a multiple choice formulation. In this notebook, we'll see if we can improve our accuracy on the CoPA task by combining each of our approaches through a voting mechanism. For voting, we'll use:\n",
    "1) LLaMA Generation\n",
    "2) Falcon Generation\n",
    "3) LLaMA MC with Bootstrapping (see [Bootstrap Ensembling Notebook](bootstrap_ensembling.ipynb))\n",
    "4) Falcon MC with Bootstrapping (see [Bootstrap Ensembling Notebook](bootstrap_ensembling.ipynb))\n",
    "5) LLaMA Log Probability Estimation\n",
    "\n",
    "Each of these methods will consitute a vote for a particular label and then we'll measure accuracy. \n",
    "\n",
    "__Note__ We're doing a lot of generations here. This notebook takes a fair bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "copa_data_set = copa_preprocessor(\"resources/copa_sample.tsv\")\n",
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "test_pool = copa_data_set[demonstration_candidates:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "\n",
    "prompts: List[str] = []\n",
    "int_labels: List[int] = []\n",
    "choices: List[Tuple[str, str]] = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    choices.append((first_choice, second_choice))\n",
    "    int_labels.append(label)\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that both of these are GREEDY decoding strategies for the different models\n",
    "llama_generation_config = {\"max_tokens\": 20, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "falcon_generation_config = {\"max_tokens\": 20, \"top_k\": 1, \"temperature\": 1.0, \"do_sample\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.35s/it]\n",
      "100%|██████████| 13/13 [02:27<00:00, 11.33s/it]\n"
     ]
    }
   ],
   "source": [
    "llama_responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = llama_model.generate(prompt_batch, llama_generation_config)\n",
    "    llama_responses.extend(process_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "falcon_responses = []\n",
    "# Falcon requires a batch size of 8 or less\n",
    "prompt_batches = split_prompts_into_batches(prompts, 8)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = falcon_model.generate(prompt_batch, falcon_generation_config)\n",
    "    falcon_responses.extend(process_generation_text(generations.generation[\"sequences\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scoring based on the generated text, by considering the rouge score of the responses using the label as the reference. We choose between the two available choices for the logical completion of the reference phrase. The model has provided a response and we treat each choice as a reference for the ROUGE metric. We take as the model's prediction the phrase with the highest ROUGE score compared to the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response], references=[first_choice])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response], references=[second_choice])\n",
    "    # We take the average of the unigram and bi-gram rouge scores for the first and second choice results.\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    # If the first score is larger we select the first choice\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_predictions = []\n",
    "for response, (first_choice, second_choice) in zip(llama_responses, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    llama_predictions.append(predicted_label)\n",
    "all_predictions.append(llama_predictions)\n",
    "\n",
    "falcon_predictions = []\n",
    "for response, (first_choice, second_choice) in zip(falcon_responses, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    falcon_predictions.append(predicted_label)\n",
    "all_predictions.append(falcon_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Choice Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mc_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\"(A|B)\", single_generation)\n",
    "        # If you find an A or B in the answer use the first occurence. Otherwise randomly select one\n",
    "        if len(generation_text) == 0:\n",
    "            print(f\"Selecting Randomly. No selection match was found in: {single_generation}\")\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else choice([\"A\", \"B\"])\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 8\n",
    "\n",
    "# Note that we use a GREEDY decoding strategies for the model.\n",
    "llama_generation_config = {\"max_tokens\": 4, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "\n",
    "all_llama_responses = []\n",
    "\n",
    "number_of_voters = 7\n",
    "for voter_number in range(number_of_voters):\n",
    "    print(f\"Starting MC Choice Response Number: {voter_number + 1}\")\n",
    "    demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "    demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "    prompts = []\n",
    "    choices = []\n",
    "    for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "        choices.append((first_choice, second_choice))\n",
    "        prompts.append(create_mc_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "\n",
    "    llama_responses = []\n",
    "    prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "    for prompt_batch in tqdm(prompt_batches):\n",
    "        generations = llama_model.generate(prompt_batch, llama_generation_config)\n",
    "        llama_responses.extend(process_mc_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "    all_llama_responses.append(llama_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_predictions = []\n",
    "for response_tuple in zip(*all_llama_responses):\n",
    "    predicted_label = Counter(response_tuple).most_common(1)[0][0]\n",
    "    llama_predictions.append(0) if predicted_label == \"A\" else llama_predictions.append(1)\n",
    "all_predictions.append(llama_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:08<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 8\n",
    "\n",
    "# Note that we use a GREEDY decoding strategies for the model.\n",
    "falcon_generation_config = {\"max_tokens\": 4, \"top_k\": 1, \"temperature\": 1.0, \"do_sample\": False}\n",
    "\n",
    "all_falcon_responses = []\n",
    "\n",
    "number_of_voters = 7\n",
    "for voter_number in range(number_of_voters):\n",
    "    print(f\"Starting MC Choice Response Number: {voter_number + 1}\")\n",
    "    demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "    demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "    prompts = []\n",
    "    choices = []\n",
    "    for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "        choices.append((first_choice, second_choice))\n",
    "        prompts.append(create_mc_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "\n",
    "    falcon_responses = []\n",
    "    prompt_batches = split_prompts_into_batches(prompts, 1)\n",
    "    for prompt_batch in tqdm(prompt_batches):\n",
    "        generations = falcon_model.generate(prompt_batch, falcon_generation_config)\n",
    "        falcon_responses.extend(process_mc_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "    all_falcon_responses.append(falcon_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_predictions = []\n",
    "for response_tuple in zip(*all_falcon_responses):\n",
    "    predicted_label = Counter(response_tuple).most_common(1)[0][0]\n",
    "    falcon_predictions.append(0) if predicted_label == \"A\" else falcon_predictions.append(1)\n",
    "all_predictions.append(falcon_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 15043, 445, 338, 263, 1243]\n",
      "Decoded Tokens: <s> Hello this is a test\n",
      "Last Layer Name: output\n",
      "Endline Token Id: 13\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer prepares the input of the model. LLaMA models of all sizes use the same underlying tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/david/Desktop/LLaMA2_Tokenizer\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"BOS\")\n",
    "encoded_tokens = tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = tokenizer.decode(encoded_tokens)\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "\n",
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = llama_model.module_names[-1]\n",
    "print(f\"Last Layer Name: {last_layer_name}\")\n",
    "# Get a log softmax function to compute log probabilities from the output layer.\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "endline_token_id = tokenizer.encode(\"Hello\\n\")[-1]\n",
    "print(f\"Endline Token Id: {endline_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probability_from_activations(logits: torch.Tensor, token_ids: List[int]) -> float:\n",
    "    # First we get the logprobs associated with each token, logits is n_tokens x vocabulary size\n",
    "    log_probs = log_softmax(logits.type(torch.float32))\n",
    "    # Drop the first token ID (as it corresponds to the <s> token) and add placeholder to the end\n",
    "    token_ids.pop(0)\n",
    "    token_ids.append(1)\n",
    "    # We only really care about the logprobs associated with the sentence to be completed\n",
    "    # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "    # sum the logprobs thereafter.\n",
    "    endline_index = len(token_ids) - list(reversed(token_ids)).index(endline_token_id)\n",
    "    # Turn token ids into the appropriate column indices\n",
    "    token_id_slicer = torch.Tensor(token_ids).reshape(-1, 1).type(torch.int64)\n",
    "    log_probs_per_token = log_probs.gather(1, token_id_slicer)\n",
    "    # We sum the log probabilities, except for the last one which corresponds to the as yet predicted token)\n",
    "    # and then normalize by the number of tokens (minus one for the placeholder)\n",
    "    selected_log_probs_per_token = log_probs_per_token[endline_index:-1]\n",
    "    normalized_log_prob = torch.sum(selected_log_probs_per_token) / len(selected_log_probs_per_token)\n",
    "    return normalized_log_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're running a lot of activation retrievals. Once in a while there is a json decoding or triton error. If that\n",
    "# happens, we retry the activations request.\n",
    "def get_activations_with_retries(prompt: str, layers: List[str], config: Dict[str, Any], retries: int = 5) -> Any:\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            return llama_model.get_activations(prompt, layers, config)\n",
    "        except Exception as e:  # noqa: F841\n",
    "            print(\"Something went wrong in activation retrieval...retrying\")\n",
    "    raise ValueError(\"Exceeded retry limit. Exiting Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_prompts_with_choices(prompt_batch: List[Tuple[str, Tuple[str, str]]]) -> List[str]:\n",
    "    # We want to complete our prompt with the two possible choices and score those completions using our LM.\n",
    "    prompts_with_choices = []\n",
    "    for prompt, (first_choice, second_choice) in prompt_batch:\n",
    "        prompts_with_choices.append(f\"{prompt}{first_choice.lower()}\")\n",
    "        prompts_with_choices.append(f\"{prompt}{second_choice.lower()}\")\n",
    "    return prompts_with_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_logprobs_to_labels(logprobs: List[float]) -> Tuple[List[int], List[List[float]]]:\n",
    "    # Need to group logprobs in twos because they represent likelihoods of the two completions\n",
    "    assert len(logprobs) % 2 == 0\n",
    "    paired_logprobs = [logprobs[x : x + 2] for x in range(0, len(logprobs), 2)]\n",
    "    predicted_labels: List[int] = []\n",
    "    predicted_logprobs = []\n",
    "    for logprob_pair in paired_logprobs:\n",
    "        # Paired logprob for first and second choice together\n",
    "        predicted_labels.append(np.argmax(logprob_pair, axis=0))\n",
    "        predicted_logprobs.append(logprob_pair)\n",
    "    return predicted_labels, predicted_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    choices.append((first_choice, second_choice))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:22<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "all_logprobs = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "# We split the prompts into batches of 1 for memory management since activation retrieval is a bit heavy.\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices, 1)\n",
    "llama_generation_config = {\"max_tokens\": 1, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    # Process below only works for batches of size 1\n",
    "    assert len(prompt_batch) == 1\n",
    "    single_prompt = prompt_batch[0]\n",
    "    # The score for a sentence is the sum of log probability of each word in the sentence.\n",
    "    prompt_activations = get_activations_with_retries(single_prompt, [last_layer_name], llama_generation_config)  # type: ignore # noqa: E501\n",
    "    token_ids = tokenizer.encode(single_prompt)\n",
    "    last_layer_matrix = prompt_activations.activations[0][last_layer_name]\n",
    "    prompt_log_probs = compute_log_probability_from_activations(last_layer_matrix, token_ids)\n",
    "    all_logprobs.append(prompt_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels, _ = post_process_logprobs_to_labels(all_logprobs)\n",
    "all_predictions.append(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Voting!\n",
    "\n",
    "At long last, we have all of our predictions from all of our prompts. Now we're going to use them to vote for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for prediction_tuples, label in zip(zip(*all_predictions), int_labels):\n",
    "    majority_prediction = Counter(prediction_tuples).most_common(1)[0][0]\n",
    "    total += 1\n",
    "    if majority_prediction == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to any of the results we saw in the other notebooks ([Prompting Examples](copa_prompting_examples.ipynb) and [Bootstrap Examples](bootstrap_ensembling.ipynb)), this is the best accuracy we've gotten and the first time over 80%. This is a nice result, but it should be noted that it was quite \"expensive\" to generate so many prompts. So the increase in accuracy has to be weighed against that cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
