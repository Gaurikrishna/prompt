{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import datasets\n",
    "import kscope\n",
    "from datasets import Dataset\n",
    "from kscope import Model\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b11f3264-9c03-4114-9d56-d39a0fa63640',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'c5ea8da7-3384-4c7b-b47f-95ed1a485897',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama2-13b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation \n",
    "\n",
    "In this notebook, we're going to extract the activations from the final non-pad token of LLaMA-13B for different sets of text inputs. This is done (on the back-end) by inserting hooks into the model that allow for extraction of the model's intermediate latent representations. In this notebook, we'll vary both the layer we extract information from and the input itself to consider the affect that these choices have on the performance of a downstream task. In this case, we'll consider a sampling of the IMDB sentiment analysis task to probe these choices.\n",
    "\n",
    "To start, we need to define a configuration for the model generation. Because we only care about the activations of our input, the configuration is less important. The only thing we really need to do is set the `max_tokens` to 1 so that we don't have to worry about indexing into the right spot in our activation matrix. That is, the activations we care about will just occur in the last slot of the tensor. For a discussion of the configuration parameters see [CONFIG_README.md](../../prompting_vector_llms/CONFIG_README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\"max_tokens\": 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation \n",
    "\n",
    "Activation generation is quite easy. We can use the client to query the remote model and explore the various modules. Here, we are listing only the last 10 layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoder.layers.95.self_attn',\n",
       " 'decoder.layers.95.self_attn.dropout_module',\n",
       " 'decoder.layers.95.self_attn.qkv_proj',\n",
       " 'decoder.layers.95.self_attn.out_proj',\n",
       " 'decoder.layers.95.self_attn_layer_norm',\n",
       " 'decoder.layers.95.fc1',\n",
       " 'decoder.layers.95.fc2',\n",
       " 'decoder.layers.95.final_layer_norm',\n",
       " 'decoder.layer_norm',\n",
       " 'decoder.output_projection']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module_names[-10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the module names of interest and pass them into a `get_activations` function alongside our set of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations(activations=[{'decoder.layers.95.fc2': tensor([[ 0.0062, -0.4624,  0.4702,  ...,  0.0985,  1.2588, -0.7002],\n",
      "        [-0.2197,  0.3364, -0.1965,  ..., -0.2651,  0.0475, -0.1638]],\n",
      "       dtype=torch.float16)}, {'decoder.layers.95.fc2': tensor([[ 2.5312, -1.4609, -1.7266,  ...,  1.9453, -0.8262, -1.5234],\n",
      "        [-0.2294, -0.6396,  0.9043,  ..., -0.8140,  0.1183,  0.2437],\n",
      "        [-0.8062,  0.8320, -0.8003,  ..., -0.8530, -0.7656,  0.5718]],\n",
      "       dtype=torch.float16)}], logprobs=[[None, -7.15677547454834, -6.544065475463867], [None, -5.25247859954834, -6.673819065093994, -5.898566246032715]], text=['Hello World', 'Fizz Buzz'], tokens=[['</s>', 'Hello', ' World'], ['</s>', 'F', 'izz', ' Buzz']])\n",
      "Tensor Shape: torch.Size([2, 12288])\n",
      "Tensor Shape: torch.Size([3, 12288])\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Hello World\", \"Fizz Buzz\"]\n",
    "\n",
    "module_name = \"output\"\n",
    "\n",
    "activations = model.get_activations(prompts, [module_name], generation_config)\n",
    "pprint(activations)\n",
    "\n",
    "# We sent a batch of 2 prompts to the model.\n",
    "# So a list of length two is returned containing activations for the requested layer\n",
    "for activations_single_prompt in activations.activations:\n",
    "    # For each prompt we extract the activations associated with the target module.\n",
    "    raw_activations = activations_single_prompt[module_name]\n",
    "    # The activations should have shape (number of tokens + 1) x (activation size)\n",
    "    # For example, LLaMA-13B has an embedding dimension for the layer requested of 4096\n",
    "    print(\"Tensor Shape:\", raw_activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: In the code below, we're going to only use batch sizes of 1 to ensure memory management on the backend doesn't get out of hand and slow the model down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 383, 4981, 350, 18813]\n",
      "Decoded Tokens: ['<s>', 'F', 'izz', 'B', 'uzz']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer prepares the input of the model. LLaMA models of all sizes use the same underlying tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/david/Desktop/LLaMA2_Tokenizer\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"BOS\")\n",
    "encoded_tokens = tokenizer.encode(\"Fizz Buzz\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = [tokenizer.decode(encoded_token) for encoded_token in encoded_tokens]\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the second example, \"Fizz Buzz\", is tokenized as [\"F\", \"izz\", \"B\", 'uzz]. So we receive a tensor with 5 rows (one for each token and a final one for the next token to be generated) and 4096 columns (the hidden dimension of LLaMA-2-13B)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof of concept of the few-shot abilities of LLMs, we'll only use a small training dataset and will only perform validation using a small test subset for compute efficiency.\n",
    "\n",
    "* Training set: 100 randomly sampled training examples\n",
    "* Test set: 300 randomly sample test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.load_dataset(\"imdb\")\n",
    "train_size = 100\n",
    "test_size = 300\n",
    "n_demonstrations = 5\n",
    "\n",
    "activation_save_path = \"./resources/\"\n",
    "\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(train_size))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(test_size))])\n",
    "# We're going to be experimenting with the affect that prompting the model for the task we care about has on a\n",
    "# classifier trained on the activations in terms of performance. So we will construct demonstrations by randomly\n",
    "# selecting a set of 5 examples from the training set to serve this purpose.\n",
    "small_demonstration_set = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(n_demonstrations))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a list into a tuple of lists of a fixed size\n",
    "def batcher(prompts: List[str], batch_size: int) -> Dataset:\n",
    "    return (prompts[pos : pos + batch_size] for pos in range(0, len(prompts), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're running a lot of activation retrievals. Once in a while there is a json decoding or triton error. If that\n",
    "# happens, we retry the activations request.\n",
    "def get_activations_with_retries(prompt: str, layers: List[str], config: Dict[str, Any], retries: int = 5) -> Any:\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            return model.get_activations(prompt, layers, config)\n",
    "        except Exception as e:  # noqa: F841\n",
    "            print(\"Something went wrong in activation retrieval...retrying\")\n",
    "    raise ValueError(\"Exceeded retry limit. Exiting Process\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text Activations\n",
    "\n",
    "Let's start by getting the activations associated with the raw review text. We'll do activations for the text coupled with a prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_activations(\n",
    "    split: str,\n",
    "    inputs: List[str],\n",
    "    labels: List[int],\n",
    "    model: Model,\n",
    "    module_name: str,\n",
    "    pickle_name: str,\n",
    "    batch_size: int = 1,\n",
    ") -> None:\n",
    "    print(\"Generating Activations with Prompts: \" + split)\n",
    "\n",
    "    parsed_activations = []\n",
    "    for input_batch in tqdm(batcher(inputs, batch_size)):\n",
    "        # Getting activations for each input batch. For an example of how get_activations works, see beginning of this\n",
    "        # notebook.\n",
    "        raw_activations = model.get_activations(input_batch, [module_name], generation_config).activations\n",
    "        for raw_activation in raw_activations:\n",
    "            # We will be performing classification on the last token non-pad token of the sequence. This is common\n",
    "            # practice for autoregressive models (e.g. OPT, Falcon, LLaMA-2). So we only keep the last row of the\n",
    "            # activation matrix.\n",
    "            parsed_activations.append(raw_activation[module_name][-1].float())\n",
    "\n",
    "    cached_activations = {\"activations\": parsed_activations, \"labels\": labels}\n",
    "\n",
    "    with open(os.path.join(activation_save_path, f\"{split}{pickle_name}.pkl\"), \"wb\") as handle:\n",
    "        pickle.dump(cached_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Activations with Prompts: train\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Generating Activations with Prompts: test\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Batch Number 7 Complete\n",
      "Batch Number 8 Complete\n",
      "Batch Number 9 Complete\n",
      "Batch Number 10 Complete\n",
      "Batch Number 11 Complete\n",
      "Batch Number 12 Complete\n",
      "Batch Number 13 Complete\n",
      "Batch Number 14 Complete\n",
      "Batch Number 15 Complete\n",
      "Batch Number 16 Complete\n",
      "Batch Number 17 Complete\n",
      "Batch Number 18 Complete\n"
     ]
    }
   ],
   "source": [
    "module_names = [\"output\", \"output\", \"output\", \"output\"]\n",
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "assert len(module_names) == len(layer_numbers)\n",
    "\n",
    "for modulel_name, layer_number in zip(module_name, layer_numbers):\n",
    "    generate_dataset_activations(\n",
    "        \"train\", small_train_dataset[\"text\"], train_labels, model, module_name, f\"_activations_demo_{layer_number}\"\n",
    "    )\n",
    "    generate_dataset_activations(\n",
    "        \"test\", small_test_dataset[\"text\"], test_labels, model, module_name, f\"_activations_demo_{layer_number}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Conditioned Activations\n",
    "\n",
    "Now let's generate activations pre-conditioned with an instruction and a few demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(instruction: str, demonstration_set: Dataset) -> str:\n",
    "    label_int_to_str = {0: \"negative\", 1: \"positive\"}\n",
    "    demonstration = f\"{instruction}\"\n",
    "    demo_texts = demonstration_set[\"text\"]\n",
    "    demo_labels = demonstration_set[\"label\"]\n",
    "    for text, label in zip(demo_texts, demo_labels):\n",
    "        # truncate the text in case it is very long (cutting the first part of text)\n",
    "        split_text = text.split(\" \")\n",
    "        if len(split_text) > 64:\n",
    "            text = \" \".join(split_text[-128:])\n",
    "        demonstration = f\"{demonstration}\\nText: {text}\\nSentiment: {label_int_to_str[label]}\"\n",
    "    return f\"{demonstration}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(texts: List[str], demonstration: str) -> List[str]:\n",
    "    return [f\"{demonstration}Text: {text} The sentiment is\" for text in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the demonstration structure (based on 5 examples) and what each prompt passed to OPT looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration:\n",
      "Classify the sentiment of the text.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is positive.\n",
      "\n",
      "Text: a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie. The sentiment is positive.\n",
      "\n",
      "Text: of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do. The sentiment is negative.\n",
      "\n",
      "Text: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition. The sentiment is positive.\n",
      "\n",
      "Text: having to do with verisimilitude. However, it's dragged down from greatness by its insistence on trendy distractions, which culminate in a long scene where a horrible five-day stomach flu makes the rounds in the household. We must endure pointless fantasy sequences, initiated by the imaginary ringleader Leary. Whose existence, by the way, is finally reminiscent of the Brad Pitt character in *Fight Club*. And this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.) The sentiment is negative.\n",
      "\n",
      "\n",
      "Prompt Example:\n",
      "Classify the sentiment of the text.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is positive.\n",
      "\n",
      "Text: a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie. The sentiment is positive.\n",
      "\n",
      "Text: of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do. The sentiment is negative.\n",
      "\n",
      "Text: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition. The sentiment is positive.\n",
      "\n",
      "Text: having to do with verisimilitude. However, it's dragged down from greatness by its insistence on trendy distractions, which culminate in a long scene where a horrible five-day stomach flu makes the rounds in the household. We must endure pointless fantasy sequences, initiated by the imaginary ringleader Leary. Whose existence, by the way, is finally reminiscent of the Brad Pitt character in *Fight Club*. And this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.) The sentiment is negative.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is\n"
     ]
    }
   ],
   "source": [
    "demonstration = create_demonstrations(\"Classify the sentiment of the text.\", small_demonstration_set)\n",
    "print(f\"Demonstration:\\n{demonstration}\")\n",
    "\n",
    "train_prompts = create_prompts(small_train_dataset[\"text\"], demonstration)\n",
    "test_prompts = create_prompts(small_test_dataset[\"text\"], demonstration)\n",
    "print(f\"Prompt Example:\\n{train_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_names = [\"output\", \"output\", \"output\", \"output\"]\n",
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "assert len(module_names) == len(layer_numbers)\n",
    "\n",
    "for modulel_name, layer_number in zip(module_name, layer_numbers):\n",
    "    generate_dataset_activations(\n",
    "        \"train\", train_prompts, train_labels, model, module_name, f\"_activations_with_prompts_demo_{layer_number}\"\n",
    "    )\n",
    "    generate_dataset_activations(\n",
    "        \"test\",\n",
    "        test_prompts,\n",
    "        test_labels,\n",
    "        model,\n",
    "        module_name,\n",
    "        f\"_activations_with_prompts_demo_{layer_number}\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these activations saved, the next step is to train a simple classifier on top of them in order to perform the sentiment classification. This is done in the `train_on_activations.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
